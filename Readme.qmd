---
title: "Supporting Material S2: Example analysis of a dataset with two factors using prolfquapp"
format:
  html:
    toc: true
editor: visual
editor_options: 
  chunk_output_type: console
execute: 
  output: false
  eval: true
  cache: true
engine: knitr
---

## Example analysis using prolfquapp

*prolfquapp* \[<https://github.com/prolfqua/prolfquapp>\] is an R package which can be used to perform differential expression analysis of proteins. It builds upon the core functionalities of the $prolfqua$ package \[<https://github.com/fgcz/prolfqua>\].

The data used in this analysis we sourced from [MassIVE Reanalysis - RMSV000000696.1](https://massive.ucsd.edu/ProteoSAFe/reanalysis.jsp?task=b88e6fe3f3564773be62eefde7122127). Two key files `report.tsv` and `msstats.tsv` were processed for this study, each approximately 17GB in size. These files were produced from the raw files using the FragPipe DIA workflow <https://fragpipe.nesvilab.org/docs/tutorial_DIA.html>, and the preprocessing is discussed in more detail in the publication by D.Kohler et al. <https://doi.org/10.1038/s41596-024-01000-3>.

The dataset has $187$ samples and for more information about the dataset, we refer you to <https://pdc.cancer.gov/pdc/study/PDC000200>. The FragPipe DIA workflow identified and quantified $8613$ Proteins.

We have chosen this dataset is to demonstrate the performance and functionality of the $prolfquapp$ package to analyse large datasets and experiments with factorial designs.

# Replicating the document

The web-resource <https://fgcz-proteomics.uzh.ch/public/wew_prolfquapp/DEA_large_example/> contains copies of the input `report.tsv` file and `msstats.tsv` file, and examples of `.yml` files and annotation XLSX files which the $prolfquapp$ application requires as input. It also contains the outputs generated by $prolfquapp$.

To recreate the analysis on your computer you will need the docker application. Furthermore, you need to download the content of the folder. Then run:

By executing:

```{bash}
#| label: exmpleusgae
#| eval: false
./p_docker.sh quarto render Readme.qmd --to html
```

you will execute all the code blocks in this document.  The shell script `p_docker.sh` file is used to set up the docker container and run the analysis using the $prolfquapp$ installation provided by the docker image.

The folders starting with `FragPipe_` contain the quantification results. Folders starting with `QC_` contain the output of the `prolfqua_qc` application while folders starting with `DEA_` contain the output of the `prolfqua_dea` application.

The latest version of the document is available at:

[https://github.com/wolski/DEA_large_dataset_example](https://github.com/wolski/DEA_large_dataset_example)

## Creating a Subset of 20 files

Using the R code below we created a subset containing data only from $20$ samples out of the $187$ samples.

```{r}
#| label: createf20datamsstats
#| eval: false
#| 
library(readr)
alld <- readr::read_csv("FragPipe_f187_msstats/msstats.csv")
Runs <- unique(alld$Run)
Runs20 <- sample(Runs, 20)
ms20 <- alld[alld$Run %in% Runs20,]
readr::write_csv(ms20, file = "FragPipe_f20_msstats/msstats20.csv")

```

```{r}
#| label: createf20dataDIANN
#| eval: false
#| 
alldiann <- readr::read_tsv("FragPipe_f187_diann/report.tsv")
Runs <- alldiann$File.Name |> unique()
Runs20 <- sample(Runs , 20)

ms20diann <- alldiann[alldiann$File.Name %in% Runs20,]
readr::write_tsv(ms20diann, file = "FragPipe_f20_diann/f20_report.tsv")
```

## Duplicating and triplicating the dataset

To asses the runtime and  memmory usage for larger experiments we duplicte and triplicate the original dataset.

```{r}
#| label: createf2XdataDIANN
#| eval: false
#| 

alld <- readr::read_tsv("FragPipe_f187_diann/report.tsv")
alld2 <- alld
alld2$Run <- paste0(alld2$Run, "_V2")
dd <- unique(alld2$File.Name)
alld2$File.Name <- gsub("\\.mzML", "_V2\\.mzML", alld2$File.Name)
alldx <- dplyr::bind_rows(alld, alld2)
readr::write_tsv(alldx, "FragPipe_f374_diann/x2_report.tsv")
```

```{r}
#| label: createf3XdataDIANN
#| eval: false
#| 

alld3 <- alld
alld3$Run <- paste0(alld3$Run, "_V3")
alld3$File.Name <- gsub("\\.mzML", "_V3\\.mzML", alld3$File.Name)
alld <- dplyr::bind_rows(alldx, alld3)
dir.create("FragPipe_f561_diann")
readr::write_tsv(alld, "FragPipe_f561_diann/x3_report.tsv")

```

We here define a helper function to compute the run-time and memory usage of the QC and DEA analysis.

```{r}
#! label: computeruntime.
runtime_DEA <- function(log_file) {
  datafLOG <- read.table(log_file, header = TRUE, sep = "", fill = TRUE,
                            comment.char = "", check.names = FALSE,
                            skip = 0)
  datafLOG <- datafLOG[!grepl("^%CPU", datafLOG$`%CPU`), ]
  datafLOG <- datafLOG[-nrow(datafLOG),]
  datafLOG$TIME  <- as.numeric(lubridate::hms(datafLOG$TIME))
  datafLOG$GB <- as.numeric(datafLOG$RSS)/(1024*1024)
  res <- list(data = datafLOG, maxGB = max(datafLOG$GB),
              maxTime = max(datafLOG$TIME) / 60)
  return(res)
}

```



## Setup

We start by cleaning the outputs of previous runs.

```{bash}
#| label: cleanup
#| echo: false
#| eval: true
rm -f *.Rmd
rm -f prolfqua_*.sh
rm -rf DEA_*
rm -rf qc_*
rm -f FragPipe_f187_msstats/*.log
rm -f FragPipe_f20_msstats/*.log
rm -f FragPipe_f187_diann/*.log
rm -f FragPipe_f20_diann/*.log
rm -f FragPipe_f374_diann/*.log
rm -f FragPipe_f561_diann/*.log

```

Next we deploy the shell scripts provided by the $prolfquapp$ package.

```{bash}
#| label: copyscripts
#| eval: true

R --vanilla -e "prolfquapp::copy_shell_script(workdir = '.')"
```

## DIANN reports.tsv file for 20 samples

Create dataset annotation file using information in the `mstats.tsv` file.

```{bash}
#| label: deadiann_f20
#| eval: true
./prolfqua_dataset.sh -s DIANN -i FragPipe_f20_diann \
  -d FragPipe_f20_diann/dataset_diann_example.xlsx
```

Create QC and sample size estimation report, and generate XLSX file with protein abundance estimates using Tukeys median polish and iBAQ values. The outputs are stored in the `qc_dir_f20_diann` folder.

```{bash}
#| label: qcf20diann
#| eval: true
./prolfqua_qc.sh -s DIANN -i FragPipe_f20_diann \
  -d dataset_all_parallel.xlsx -o qc_dir_f20_diann
```

### Example of parallel group design, with missingness modelling


```{bash}
#| label: paralellf20diann
./prolfqua_dea.sh -s DIANN -i FragPipe_f20_diann -d dataset_all_parallel.xlsx \
  -y config_model_missing_vsn.yml -w f20_diann_with_subject
```

Same analysis as before, but we filter for at least 2 peptides per protein.


```{bash}
#| label: paralellf20diann2peptides
./prolfqua_dea.sh -s DIANN -i FragPipe_f20_diann -d dataset_all_parallel.xlsx \
  -y config_2_or_more_missing.yml -w f20_diann_with_subject_2PEP
```


### Example of factorial design, and no missingness modelling

```{bash}
#| label: factorialf20diann
./prolfqua_dea.sh -s DIANN -i FragPipe_f20_diann \
  -d dataset_all_interaction_no_Subject.xlsx \
  -y config_vsn.yml -w f20_diann_with_interaction
```

We rerun the same analysis but filter for two peptides per protein.
The filter parameters are defined in the yaml file 

```
ext_reader:
  extra_args: list(q_value = 0.01, hierarchy_depth = 1, nr_peptides = 2)
  preprocess: prolfquapp::preprocess_DIANN
  get_files: prolfquapp::get_DIANN_files
```


```{bash}
#| label: factorialf20diann2Pep
./prolfqua_dea.sh -s DIANN -i FragPipe_f20_diann \
  -d dataset_all_interaction_no_Subject.xlsx \
  -y config_2_or_more.yml -w f20_diann_with_interaction_2Peptides
```



### Example PEPTIDE level analysis

Note that we use the `DIANN_PEPTIDE` option here.

```{bash}
#| label: paralellf20peptide
./prolfqua_dea.sh -s DIANN_PEPTIDE -i FragPipe_f20_diann \
  -d dataset_all_parallel.xlsx -y config_model_missing_vsn.yml \
  -w f20_diann_peptide_with_subject
```

```{r}
#| label: f20diannTiming
#| output: true
#| 
undebug(runtime_DEA)
resDS <- runtime_DEA("FragPipe_f20_diann/prolfqua_logMemUsage_dataset.log")
resQ <- runtime_DEA("FragPipe_f20_diann/prolfqua_logMemUsage_qc.log")
resD1 <- runtime_DEA("FragPipe_f20_diann/prolfqua_logMemUsage_dea_2.log")
resD2 <- runtime_DEA("FragPipe_f20_diann/prolfqua_logMemUsage_dea_1.log")
resDPEP <- runtime_DEA("FragPipe_f20_diann/prolfqua_logMemUsage_dea.log")

timing <- data.frame(name = c("Dataset", "QC", "DEA1", "DEA2","DEAPEP"),
  RAM_GB = c(resDS$maxGB, resQ$maxGB, resD1$maxGB , resD2$maxGB, resDPEP$maxGB),
  Time_min = c(resDS$maxTime, resQ$maxTime, resD1$maxTime , resD2$maxTime, resDPEP$maxTime))

knitr::kable(timing,caption = "f20 DIANN report tsv input")
```

## DIANN reports.tsv file for 187 samples

```{bash}
#| label: qcdiann187makedataset
#| 

./prolfqua_dataset.sh -s DIANN -i FragPipe_f187_diann \
  -d FragPipe_f187_diann/dataset_diann_example.xlsx
```

```{bash}
#| label: qcdiann187qc
#| 

./prolfqua_qc.sh -s DIANN -i FragPipe_f187_diann \
  -d dataset_all_parallel.xlsx -o qc_dir_f187_diann
```

```{bash}
#| label: deadiann187

./prolfqua_dea.sh -s DIANN -i FragPipe_f187_diann \
  -d dataset_all_interaction_no_Subject.xlsx \
  -y config_vsn.yml -w f187_diann_with_interaction

```

```{r}
#| label: alldiannTiming
#| output: true

resDS <- runtime_DEA("FragPipe_f187_diann/prolfqua_logMemUsage_dataset.log")
resQ <- runtime_DEA("FragPipe_f187_diann/prolfqua_logMemUsage_qc.log")
resD2 <- runtime_DEA("FragPipe_f187_diann/prolfqua_logMemUsage_dea.log")

timing <- data.frame(name = c("Dataset", "QC", "DEA1"),
  RAM_GB = c(resDS$maxGB, resQ$maxGB,resD2$maxGB),
  Time_min = c(resDS$maxTime, resQ$maxTime, resD2$maxTime))

knitr::kable(timing,caption = "f187 DIANN report tsv input")
```

## MSstats.tsv formatted output for dataset with 20 files.

$FragPipe$ can reformat the $DIA-NN$ output in report.tsv to $MSstats$ compatible output. We show here that you can run a similar analysis starting from this output using prolfqua.

Create dataset annotation file using information in the `mstats.tsv` file.

```{bash}
#| label: makedataset20
./prolfqua_dataset.sh -s MSSTATS -i FragPipe_f20_msstats \
  -d FragPipe_f20_msstats/dataset_msstats20_example.xlsx
./prolfqua_qc.sh -s MSSTATS_FP_DIA -i FragPipe_f20_msstats \
  -d dataset_all_parallel.xlsx -o qc_dir_msstats20

```

```{bash}
#| label: dea20msstats
./prolfqua_dea.sh -s MSSTATS_FP_DIA -i FragPipe_f20_msstats \
  -d dataset_all_parallel.xlsx -y config_model_missing_vsn.yml \
  -w f20_msstats_parallel_with_subject
  
./prolfqua_dea.sh -s MSSTATS_FP_DIA -i FragPipe_f20_msstats \
  -d dataset_all_interaction_no_Subject.xlsx \
  -y config_vsn.yml -w f20_msstats_with_interaction_no_subject
```

```{r}
#| label: msstats20_timing
#| echo: true
#| output: true
undebug(runtime_DEA)
resDS <- runtime_DEA("FragPipe_f20_msstats/prolfqua_logMemUsage_dataset.log")
resQ <- runtime_DEA("FragPipe_f20_msstats/prolfqua_logMemUsage_qc.log")
resD1 <- runtime_DEA("FragPipe_f20_msstats/prolfqua_logMemUsage_dea_1.log")
resD2 <- runtime_DEA("FragPipe_f20_msstats/prolfqua_logMemUsage_dea.log")

timing <- data.frame(name = c("Dataset", "QC", "DEA1", "DEA2"),
  RAM_GB = c(resDS$maxGB, resQ$maxGB, resD1$maxGB , resD2$maxGB),
  Time_min = c(resDS$maxTime, resQ$maxTime, resD1$maxTime , resD2$maxTime))

knitr::kable(timing,caption = "f20 MSstats csv input")
```

## DIANN report.tsv with 374 samples.

This dataset was create by duplicating the dataset with 187 samples.

```{bash}
#| label: qcdiannf374makedataset
#| 

./prolfqua_dataset.sh -s DIANN -i FragPipe_f374_diann \
  -d FragPipe_f374_diann/dataset_diann_example.xlsx
```

```{bash}
#| label: qcdiannf374
#| 

./prolfqua_qc.sh -s DIANN -i FragPipe_f374_diann \
  -d dataset_all_parallel.xlsx -o qc_dir_f374_diann
```

```{bash}
#| label: deadiannf374

./prolfqua_dea.sh -s DIANN -i FragPipe_f374_diann \
  -d dataset_all_interaction_no_Subject.xlsx \
  -y config_vsn.yml -w f374_diann_with_interaction

```

```{r}
#| label: alldiannTiming374
#| output: true

resDS <- runtime_DEA("FragPipe_f374_diann/prolfqua_logMemUsage_dataset.log")
resQ <- runtime_DEA("FragPipe_f374_diann/prolfqua_logMemUsage_qc.log")
resD2 <- runtime_DEA("FragPipe_f374_diann/prolfqua_logMemUsage_dea.log")

timing <- data.frame(name = c("Dataset", "QC", "DEA1"),
  RAM_GB = c(resDS$maxGB, resQ$maxGB,resD2$maxGB),
  Time_min = c(resDS$maxTime, resQ$maxTime, resD2$maxTime))

knitr::kable(timing,caption = "f374 DIANN report tsv input.")
```

## DIANN report.tsv with 561 samples.

This dataset was create by duplicating the dataset with 187 samples.

```{bash}
#| label: qcdiannf561makedataset
#| 

./prolfqua_dataset.sh -s DIANN -i FragPipe_f561_diann \
  -d FragPipe_f561_diann/dataset_diann_example.xlsx
```

```{bash}
#| label: qcdiannf561
#| 

./prolfqua_qc.sh -s DIANN -i FragPipe_f561_diann \
  -d dataset_all_parallel.xlsx -o qc_dir_f561_diann
```

```{bash}
#| label: deadiannf561

./prolfqua_dea.sh -s DIANN -i FragPipe_f561_diann \
  -d dataset_all_interaction_no_Subject.xlsx \
  -y config_vsn.yml -w f561_diann_with_interaction

```

```{r}
#| label: alldiannTiming561
#| output: true

resDS <- runtime_DEA("FragPipe_f561_diann/prolfqua_logMemUsage_dataset.log")
resQ <- runtime_DEA("FragPipe_f561_diann/prolfqua_logMemUsage_qc.log")
resD2 <- runtime_DEA("FragPipe_f561_diann/prolfqua_logMemUsage_dea.log")

timing <- data.frame(name = c("Dataset", "QC", "DEA1"),
  RAM_GB = c(resDS$maxGB, resQ$maxGB,resD2$maxGB),
  Time_min = c(resDS$maxTime, resQ$maxTime, resD2$maxTime))

knitr::kable(timing,caption = "f561 DIANN report tsv input")
```



# R version and session information

```{r}
#| label: sessionInfo
#| echo: true
#| output: true
#| eval: true

pander::pander(sessionInfo())
```
